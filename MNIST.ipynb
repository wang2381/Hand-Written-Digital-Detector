{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wang\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000 42000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_df), len(train_df))\n",
    "train_df.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = train_df['label']\n",
    "train_df = train_df.drop(train_df.columns[0], axis=1)\n",
    "label.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set null value:0\n",
      "Test set null value:0\n"
     ]
    }
   ],
   "source": [
    "#no missing values in either training or testing set\n",
    "\n",
    "print('Training set null value:{0}'.format(np.sum(np.sum(train_df.isnull()))))\n",
    "print('Test set null value:{0}'.format(np.sum(np.sum(test_df.isnull()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "train_df = train_df /255.0\n",
    "test_df = test_df /255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the image np array into 4 dims \n",
    "train_df = train_df.values.reshape(-1,28,28,1)\n",
    "test_df = test_df.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28, 1) (28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "class LabelOneHotEncoder():\n",
    "    def __init__(self):\n",
    "        self.ohe = OneHotEncoder()\n",
    "        self.le = LabelEncoder()\n",
    "    def fit_transform(self, x):\n",
    "        features = self.le.fit_transform( x)\n",
    "        return self.ohe.fit_transform( features.reshape(-1,1))\n",
    "    def transform( self, x):\n",
    "        return self.ohe.transform( self.la.transform( x.reshape(-1,1)))\n",
    "    def inverse_transform( self, x):\n",
    "        return self.le.inverse_transform( self.ohe.inverse_transform( x))\n",
    "    def inverse_labels( self, x):\n",
    "        return self.le.inverse_transform( x)\n",
    "\n",
    "lohe = LabelOneHotEncoder()\n",
    "label_cat = lohe.fit_transform(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<42000x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 42000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_df, label, test_size = 0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (37800, 28, 28, 1)\n",
      "Y_train.shape:  (37800,)\n",
      "X_val.shape:  (4200, 28, 28, 1)\n",
      "Y_val.shape:  (4200,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape: ', X_train.shape)\n",
    "print('Y_train.shape: ', Y_train.shape)\n",
    "print('X_val.shape: ', X_val.shape)\n",
    "print('Y_val.shape: ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37800, 784) (37800,)\n"
     ]
    }
   ],
   "source": [
    "X_train_vec = X_train.reshape([X_train.shape[0], -1])\n",
    "Y_train = Y_train.reshape(-1)\n",
    "print(X_train_vec.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smt=SMOTE(ratio='auto', random_state=10, k=None, k_neighbors=5, m=None, m_neighbors=10, out_step=0.5, kind='regular', svm_estimator=None, n_jobs=-1)\n",
    "#SMOTE\n",
    "X_train_af, Y_train_af=smt.fit_sample(X_train_vec, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9    4216\n",
      "8    4216\n",
      "7    4216\n",
      "6    4216\n",
      "5    4216\n",
      "4    4216\n",
      "3    4216\n",
      "2    4216\n",
      "1    4216\n",
      "0    4216\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#balanced labels after SMOTE\n",
    "\n",
    "print(pd.Series(Y_train_af).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding to sparse matrices for label\n",
    "Y_val = lohe.fit_transform(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42160, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape to 4 dimensional array for CNN input\n",
    "X_train = X_train_af.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42160"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation using manual transformation (Input X_train)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage import affine_transform\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "#the mean aspect ratio\n",
    "anisotropy = 2.15\n",
    "\n",
    "def build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    rotation        = np.deg2rad(rotation)\n",
    "    shear           = np.deg2rad(shear)\n",
    "    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n",
    "    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n",
    "    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n",
    "    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "# Compute the coordinate transformation required to center the pictures\n",
    "def center_transform(affine, input_shape):\n",
    "    hi, wi = float(input_shape[0]), float(input_shape[1])\n",
    "    ho, wo = float(img_shape[0]), float(img_shape[1])\n",
    "    top, left, bottom, right = 0, 0, hi, wi\n",
    "    if wi/hi/anisotropy < wo/ho: # input image too narrow, extend width\n",
    "        w     = hi*wo/ho*anisotropy\n",
    "        left  = (wi-w)/2\n",
    "        right = left + w\n",
    "    else: # input image too wide, extend height\n",
    "        h      = wi*ho/wo/anisotropy\n",
    "        top    = (hi-h)/2\n",
    "        bottom = top + h\n",
    "    center_matrix   = np.array([[1, 0, -ho/2], [0, 1, -wo/2], [0, 0, 1]])\n",
    "    scale_matrix    = np.array([[(bottom - top)/ho, 0, 0], [0, (right - left)/wo, 0], [0, 0, 1]])\n",
    "    decenter_matrix = np.array([[1, 0, hi/2], [0, 1, wi/2], [0, 0, 1]])\n",
    "    return np.dot(np.dot(decenter_matrix, scale_matrix), np.dot(affine, center_matrix))\n",
    "\n",
    "# Apply an affine transformation to an image represented as a numpy array.\n",
    "def transform_img(x, affine):\n",
    "    matrix   = affine[:2,:2]\n",
    "    offset   = affine[:2,2]\n",
    "    x        = np.moveaxis(x, -1, 0)\n",
    "    channels = [affine_transform(channel, matrix, offset, output_shape=img_shape[:-1], order=1,\n",
    "                                 mode='constant', cval=np.average(channel)) for channel in x]\n",
    "    return np.moveaxis(np.stack(channels, axis=0), 0, -1)\n",
    "\n",
    "# Read an image for training, i.e. including a random affine transformation\n",
    "def Training_data_preprocess(x):\n",
    "    \n",
    "    t  = build_transform(\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(0.9, 1.0),\n",
    "            random.uniform(0.9, 1.0),\n",
    "            random.uniform(-0.05*img_shape[0], 0.05*img_shape[0]),\n",
    "            random.uniform(-0.05*img_shape[1], 0.05*img_shape[1]))\n",
    "    t  = center_transform(t, x.shape)\n",
    "    x  = transform_img(x, t)\n",
    "    x -= np.mean(x, keepdims=True)\n",
    "    x /= np.std(x, keepdims=True) + K.epsilon()\n",
    "    return x  \n",
    "\n",
    "# Read an image for validation, i.e. without data augmentation.\n",
    "def Validation_data_preprocess(x):\n",
    "    \n",
    "    t  = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    t  = center_transform(t, x.shape)\n",
    "    x  = transform_img(x, t)\n",
    "    x -= np.mean(x, keepdims=True)\n",
    "    x /= np.std(x, keepdims=True) + K.epsilon()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Generator():\n",
    "    #set data augmentation size = aug_batch_size*len(X_train) \n",
    "    aug_batch_size = 5\n",
    "\n",
    "    #initiate matrix for data augmentation and transfermation\n",
    "    X_train_transf = np.zeros((aug_batch_size * len(X_train), len(X_train[0]), len(X_train[0,0]), 1))\n",
    "    Y_train_transf = np.zeros((aug_batch_size * Y_train_af.shape[0]))\n",
    "\n",
    "\n",
    "    j = 0\n",
    "    for i in range(len(X_train)):\n",
    "        for _ in range(aug_batch_size):\n",
    "\n",
    "            X_train_transf[j,:,:,:] = Training_data_preprocess(X_train[i])\n",
    "            Y_train_transf[j] = Y_train_af[i]\n",
    "            j+=1\n",
    "    #One-hot encoding to sparse matrices for label\n",
    "    Y_train_transf = lohe.fit_transform(Y_train_transf)\n",
    "    return X_train_transf, Y_train_transf\n",
    "    \n",
    "    \n",
    "    \n",
    "#validation data preprocessing\n",
    "X_val_transf = np.zeros(X_val.shape)\n",
    "for i in range(len(X_val)):\n",
    "    X_val_transf[i,:,:,:] = Validation_data_preprocess(X_val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transf, Y_train_transf = Generator()\n",
    "\n",
    "#Data augmentation Using ImageDataGenerator (Input X_train)\n",
    "\n",
    "image_gen = ImageDataGenerator(\n",
    "        '''\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=False,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = False, # Randomly zoom image \n",
    "        width_shift_range=False,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=False,  # randomly shift images vertically (fraction of total height)\n",
    "        shear_range=False,\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False  # randomly flip images\n",
    "        '''\n",
    "        )\n",
    "\n",
    "image_gen.fit(X_train_transf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210800, 28, 28, 1)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building regular CNN\n",
    "\n",
    "from keras.engine.topology import Input\n",
    "from keras.layers import BatchNormalization, Concatenate, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "img_shape  = (28,28,1)\n",
    "\n",
    "def build_model():\n",
    "    kwargs     = {'activation':'relu', 'padding':'same'}\n",
    "\n",
    "    inp        = Input(shape=img_shape)\n",
    "\n",
    "    x = inp\n",
    "\n",
    "    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "                     \n",
    "    return Model(inp,x)\n",
    "\n",
    "model = build_model()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history =model.fit_generator(image_gen.flow(X_train, Y_train.toarray(),\n",
    "          batch_size=batch_size),\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, Y_val.toarray()),\n",
    "                            steps_per_epoch=33600 // batch_size)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 14, 14, 64)   320         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 14, 14, 64)   256         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 14, 14, 64)   36928       batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 14, 14, 64)   256         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 14, 14, 64)   36928       batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 14, 14, 64)   256         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 14, 14, 64)   16448       batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 14, 14, 64)   256         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 14, 14, 64)   16448       batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 14, 14, 64)   256         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 14, 14, 64)   36928       batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 14, 14, 64)   256         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 14, 14, 64)   4160        batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 14, 14, 64)   0           batch_normalization_134[0][0]    \n",
      "                                                                 conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 64)   0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 14, 14, 64)   256         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 14, 14, 64)   16448       batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 14, 14, 64)   256         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 14, 14, 64)   36928       batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 14, 14, 64)   256         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 14, 14, 64)   4160        batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 14, 14, 64)   0           batch_normalization_137[0][0]    \n",
      "                                                                 conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 64)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 7, 7, 64)     0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 7, 7, 64)     256         max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 7, 7, 64)     16448       batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 7, 7, 64)     256         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 7, 7, 64)     16448       batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 7, 7, 64)     256         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 7, 7, 64)     36928       batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 7, 7, 64)     256         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 7, 7, 64)     4160        batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 7, 7, 64)     0           batch_normalization_141[0][0]    \n",
      "                                                                 conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 7, 7, 64)     0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 7, 7, 64)     256         activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 7, 7, 64)     16448       batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 7, 7, 64)     256         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 7, 7, 64)     36928       batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 7, 7, 64)     256         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 7, 7, 64)     4160        batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 7, 7, 64)     0           batch_normalization_144[0][0]    \n",
      "                                                                 conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 7, 64)     0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 7, 7, 64)     256         activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 7, 7, 64)     36928       batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 7, 7, 64)     256         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 7, 7, 64)     16448       batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 7, 7, 64)     256         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 7, 7, 64)     36928       batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 7, 7, 64)     256         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 7, 7, 64)     4160        batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 7, 7, 64)     0           batch_normalization_148[0][0]    \n",
      "                                                                 conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 7, 7, 64)     0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 7, 7, 64)     256         activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 7, 7, 64)     16448       batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 7, 7, 64)     256         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 7, 7, 64)     36928       batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 7, 7, 64)     256         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 7, 7, 64)     4160        batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 7, 7, 64)     0           batch_normalization_151[0][0]    \n",
      "                                                                 conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 7, 7, 64)     0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 3, 3, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 3, 3, 64)     256         max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 3, 3, 64)     36928       batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 3, 3, 64)     256         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 3, 3, 64)     16448       batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 3, 3, 64)     256         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 3, 3, 64)     36928       batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 3, 3, 64)     256         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 3, 3, 64)     4160        batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 3, 3, 64)     0           batch_normalization_155[0][0]    \n",
      "                                                                 conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 3, 3, 64)     0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 3, 3, 64)     256         activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 3, 3, 64)     16448       batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 3, 3, 64)     256         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 3, 3, 64)     36928       batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 3, 3, 64)     256         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 3, 3, 64)     4160        batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 3, 3, 64)     0           batch_normalization_158[0][0]    \n",
      "                                                                 conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 3, 3, 64)     0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 576)          0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          73856       flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 10)           1290        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 724,042\n",
      "Trainable params: 720,202\n",
      "Non-trainable params: 3,840\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building ResNet CNN\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Input\n",
    "from keras.layers import Activation, BatchNormalization, Concatenate, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "img_shape  = (28,28,1)\n",
    "\n",
    "\n",
    "def subblock(x, filter, **kwargs):\n",
    "    x = BatchNormalization()(x)\n",
    "    y = x\n",
    "    y = Conv2D(filter, (2, 2), **kwargs)(y) # Reduce the number of features to 'filter'\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(filter, (3, 3), **kwargs)(y) # Extend the feature field\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(K.int_shape(x)[-1], (1, 1), **kwargs)(y) # no activation # Restore the number of original features\n",
    "    y = Add()([x,y]) # Add the bypass connection\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    kwargs = {'activation':'relu','padding':'same'}\n",
    "\n",
    "    inp = Input(shape=img_shape) # 28x28x1\n",
    "    x   = Conv2D(64, (2,2), strides=2, **kwargs)(inp)\n",
    "\n",
    "    \n",
    "    for _ in range(2):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(64, (3,3), **kwargs)(x)\n",
    "\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (2,2), **kwargs)(x)\n",
    "    for _ in range(2): x = subblock(x, 64, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (2,2), **kwargs)(x) \n",
    "    for _ in range(2): x = subblock(x, 64, **kwargs)\n",
    "\n",
    "     \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3,3), **kwargs)(x) \n",
    "    for _ in range(2): x = subblock(x, 64, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3,3), **kwargs)(x)\n",
    "    for _ in range(2): x = subblock(x, 64, **kwargs)\n",
    "     \n",
    "    x             = Flatten()(x)\n",
    "    \n",
    "    x             = Dense(128, activation='relu')(x)\n",
    "    x             = Dropout(0.25)(x)\n",
    "    \n",
    "    x             = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inp,x)\n",
    "\n",
    "model = build_model()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2451/2451 [==============================] - 81s 33ms/step - loss: 0.0197 - acc: 0.9938 - val_loss: 0.0376 - val_acc: 0.9910\n",
      "Epoch 2/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.0251 - val_acc: 0.9936\n",
      "Epoch 3/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0132 - acc: 0.9959 - val_loss: 0.0227 - val_acc: 0.9938\n",
      "Epoch 4/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0328 - val_acc: 0.9926\n",
      "Epoch 5/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0318 - val_acc: 0.9938\n",
      "Epoch 6/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0375 - val_acc: 0.9898\n",
      "Epoch 7/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0088 - acc: 0.9972 - val_loss: 0.0291 - val_acc: 0.9931\n",
      "Epoch 8/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0085 - acc: 0.9975 - val_loss: 0.0195 - val_acc: 0.9945\n",
      "Epoch 9/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0341 - val_acc: 0.9938\n",
      "Epoch 10/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.0282 - val_acc: 0.9940\n",
      "Epoch 11/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0273 - val_acc: 0.9938\n",
      "Epoch 12/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0384 - val_acc: 0.9921\n",
      "Epoch 13/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0306 - val_acc: 0.9945\n",
      "Epoch 14/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0270 - val_acc: 0.9931\n",
      "Epoch 15/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0328 - val_acc: 0.9936\n",
      "Epoch 16/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0300 - val_acc: 0.9940\n",
      "Epoch 17/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0403 - val_acc: 0.9933\n",
      "Epoch 18/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0292 - val_acc: 0.9938\n",
      "Epoch 19/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0415 - val_acc: 0.9936\n",
      "Epoch 20/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0414 - val_acc: 0.9931\n",
      "Epoch 21/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0394 - val_acc: 0.9919\n",
      "Epoch 22/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0378 - val_acc: 0.9938\n",
      "Epoch 23/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0049 - acc: 0.9989 - val_loss: 0.0366 - val_acc: 0.9929\n",
      "Epoch 24/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0355 - val_acc: 0.9940\n",
      "Epoch 25/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0368 - val_acc: 0.9929\n",
      "Epoch 26/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0319 - val_acc: 0.9933\n",
      "Epoch 27/60\n",
      "2451/2451 [==============================] - 81s 33ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0423 - val_acc: 0.9938\n",
      "Epoch 28/60\n",
      "2451/2451 [==============================] - 81s 33ms/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0376 - val_acc: 0.9931\n",
      "Epoch 29/60\n",
      "2451/2451 [==============================] - 81s 33ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0501 - val_acc: 0.9914\n",
      "Epoch 30/60\n",
      "2451/2451 [==============================] - 81s 33ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0467 - val_acc: 0.9921\n",
      "Epoch 31/60\n",
      "2451/2451 [==============================] - 81s 33ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0523 - val_acc: 0.9938\n",
      "Epoch 32/60\n",
      "2451/2451 [==============================] - 81s 33ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0405 - val_acc: 0.9931\n",
      "Epoch 33/60\n",
      "2451/2451 [==============================] - 81s 33ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0385 - val_acc: 0.9943\n",
      "Epoch 34/60\n",
      "2451/2451 [==============================] - 81s 33ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0376 - val_acc: 0.9936\n",
      "Epoch 35/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0388 - val_acc: 0.9940\n",
      "Epoch 36/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0320 - val_acc: 0.9950\n",
      "Epoch 37/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0526 - val_acc: 0.9921\n",
      "Epoch 38/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0535 - val_acc: 0.9931\n",
      "Epoch 39/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0461 - val_acc: 0.9938\n",
      "Epoch 40/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0365 - val_acc: 0.9938\n",
      "Epoch 41/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0423 - val_acc: 0.9938\n",
      "Epoch 42/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0504 - val_acc: 0.9933\n",
      "Epoch 43/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0412 - val_acc: 0.9943\n",
      "Epoch 44/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0376 - val_acc: 0.9948\n",
      "Epoch 45/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0455 - val_acc: 0.9924\n",
      "Epoch 46/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0334 - val_acc: 0.9931\n",
      "Epoch 47/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0332 - val_acc: 0.9936\n",
      "Epoch 48/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0358 - val_acc: 0.9938\n",
      "Epoch 49/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0457 - val_acc: 0.9929\n",
      "Epoch 50/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0382 - val_acc: 0.9931\n",
      "Epoch 51/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0500 - val_acc: 0.9924\n",
      "Epoch 52/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0441 - val_acc: 0.9921\n",
      "Epoch 53/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0441 - val_acc: 0.9917\n",
      "Epoch 54/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0491 - val_acc: 0.9933\n",
      "Epoch 55/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0470 - val_acc: 0.9924\n",
      "Epoch 56/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0451 - val_acc: 0.9933\n",
      "Epoch 57/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0464 - val_acc: 0.9926\n",
      "Epoch 58/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0410 - val_acc: 0.9940\n",
      "Epoch 59/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0479 - val_acc: 0.9938\n",
      "Epoch 60/60\n",
      "2451/2451 [==============================] - 82s 33ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0484 - val_acc: 0.9943\n"
     ]
    }
   ],
   "source": [
    "# Fit the ResNet CNN\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 86\n",
    "\n",
    "history =model.fit_generator(image_gen.flow(X_train_transf, Y_train_transf.toarray(), batch_size = batch_size) ,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val_transf, Y_val.toarray()), steps_per_epoch = X_train_transf.shape[0]//batch_size)\n",
    "                            \n",
    "score = model.evaluate(X_val, Y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model('C:/Users/wang/Desktop/audio/digit.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-3575e5f90941>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"validation loss\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlegend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshadow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD7CAYAAABt0P8jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFVdJREFUeJzt3X+s3XV9x/Hnbel6CbY1hMwSyfyRwDskBpyitNpSyABpldC4mbjeZbNbBcYyo41ByTYICeJE6g9M2IKEiNKauJAO3KBkCWYrxcZO0NRN3uySMVFatQZapFLpbffH93vleNJ7zveee/q95X6ej6Tp+X4+30/vO5/e++q333M+38/I0aNHkSSVY95sFyBJapfBL0mFMfglqTAGvyQVxuCXpMIY/JJUmJOanBQR5wOfycwLu9ovB64HDgN3ZeaXI+Jk4B7gd4EXgD/LzJ8PtWpJ0sD6XvFHxLXAncBoV/sC4PPApcAq4MqIWAr8JbA7M1cCXwX+dthFS5IG1+SK/yng/cDXutrPBsYz8zmAiHgEWAmsAG6pz3kQ+Lt+XyAiFgLvAPYAE40ql6SyzQdOB3Zl5qHpDOwb/Jl5b0S88Rhdi4H9HccvAEu62ifb+nkHsL3BeZKk37YSeGQ6Axrd45/CAWBRx/Ei4Pmu9sm2fvYAbN68maVLl86gJEkqw969exkbG4M6P6djJsH/Q+DMiDgV+CVwAXAr8AZgDfAdYDXNruQnAJYuXcoZZ5wxg5IkqTjTvj0+7eCPiHXAazLzjojYCDxE9SbxXZn5k4j4B+Du+p7/r4F10/0akqTjp1HwZ+bTwLL69ZaO9m8C3+w69yDwgeGVKEkaJhdwSVJhDH5JKozBL0mFMfglqTAGvyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8kFcbgl6TCGPySVBiDX5IKY/BLUmH67sAVEfOA24FzgUPAhswcr/veCnyh4/RlwFqq/XafBH5Qt2/NzC8OsW5J0oCabL24FhjNzOURsQzYBFwBkJnfAy4EiIgPAM9m5raIuBj4emb+9fEpW5I0qCbBvwLYBpCZOyPivO4TIuIU4Ebggrrp7cDbIuLfgZ8BH8nMPcMpWZI0E03u8S8G9nccT0RE9z8YfwH8U2buq4+fAG7IzFXAPwNfmnGlkqShaBL8B4BFnWMy83DXOWPAnR3HDwPfql9vBX5/4AolSUPVJPh3AGsA6nv8uzs7I2IJsDAzn+lovhP4w/r1HwDfnXmpkqRhaHKPfytwSUQ8CowA6yNiIzCemfcDZwFPd435JHBXRFwDvAhsGF7JkqSZ6Bv8mXkEuLqr+YmO/l1Un/zpHPO/wEXDKFCSNFwu4JKkwhj8klQYg1+SCmPwS1JhDH5JKozBL0mFMfglqTAGvyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8kFcbgl6TC9N2BKyLmAbcD5wKHgA2ZOd7RfxvwbuCFuukKYAGwBTgZeBZYn5kHh1u6JGkQTa741wKjmbmcai/dTV39bwPek5kX1r/2A9cDWzJzJfA4cNUwi5YkDa5J8K8AtgFk5k7gvMmO+n8DZwJ3RMSOiPjz7jHAg8DFQ6tYkjQjTYJ/MbC/43giIiZvEZ0CfAn4E+Ay4JqIOKdrzAvAkuGUK0maqb73+IEDwKKO43mZebh+fRD44uT9+4h4mOq9gMkxv6p/f35oFUuSZqTJFf8OYA1ARCwDdnf0nQU8EhHzI2IB1S2exzrHAKuB7UOrWJI0I02u+LcCl0TEo8AIsD4iNgLjmXl/RGwGdgIvA1/NzP+KiJuAuyPiw8A+YN1xql+SNE19gz8zjwBXdzU/0dF/C3BL15ifUt3zlySdYFzAJUmFMfglqTAGvyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8kFcbgl6TCGPySVBiDX5IKY/BLUmEMfkkqTN+NWCJiHnA71V66h4ANmTne0f8x4IP14QOZeWNEjAA/Bv6nbv92Zl431MolSQNpsvXiWmA0M5fXe+5uAq4AiIg3A2PA+cBRYHtEbKXahP2xzLz8+JQtSRpUk1s9K4BtAJm5Ezivo+8Z4LLMnKi3aFwAvAS8HXh9RHwrIh6IiBhy3ZKkATUJ/sXA/o7jiYg4CSAzX87MfRExEhG3Ao9n5pPAHuDTmXkRcDNwz7ALlyQNpknwHwAWdY7JzMOTBxExCmyuz7mmbv5P4D6AzHyE6up/ZCgVS5JmpEnw7wDWANT3+HdPdtRhfh/w/cy8KjMn6q4bgI/W55wL/Cgzjw6zcEnSYJq8ubsVuCQiHgVGgPURsREYB+YDq4CFEbG6Pv864O+BeyLivcBh4EPDLlySNJi+wV+/aXt1V/MTHa9Hpxj63kGLkiQdPy7gkqTCGPySVBiDX5IKY/BLUmEMfkkqjMEvSYUx+CWpMAa/JBXG4Jekwhj8klQYg1+SCmPwS1JhDH5JKozBL0mFMfglqTAGvyQVxuCXpML03YErIuYBtwPnAoeADZk53tH/YeAqqi0Wb8rMf4mI04AtwMnAs8D6zDx4HOqXJE1Tkyv+tcBoZi4HPglsmuyIiKXAR4B3A+8BPh0RC4HrgS2ZuRJ4nOofBknSCaDJZusrgG0AmbkzIs7r6HsnsCMzDwGHImIcOKcec3N9zoP168/3+BrzAfbu3Tu96iWpUB15OX+6Y5sE/2Jgf8fxRESclJmHj9H3ArCkq32yrZfTAcbGxprULEl6xenAU9MZ0CT4DwCLOo7n1aF/rL5FwPMd7b/qaOtlF7AS2ANMNKhJkko3nyr0d013YJPg3wFcDnwjIpYBuzv6vgN8KiJGgYXA2cAP6jFrgK8Aq4Htvb5AfavokekWL0mFm9aV/qSRo0eP9jyh41M95wAjwHqqUB/PzPvrT/VcSfVG8c2ZeW9EvA64m+pqfx+wLjNfHKRASdJw9Q1+SdLc4gIuSSqMwS9JhTH4JakwTT7VMzSDPP6hzfra1GAuPgZ8sD58IDNvbL/KdvSbi45z/hW4LzP/sf0q29Hg+2I1cEN9+BjwV5k5J9+oazAXHwf+GDhC9cGSrbNSaIsi4nzgM5l5YVf75VRPTDgM3JWZX+7157R9xT/I4x/mql5z8WZgDHgXsBy4NCLOmZUq2zHlXHS4CTi11apmR6/vi0XAZ4H3ZeYy4GngtNkosiW95uK1VHmxHLgU+MKsVNiiiLgWuBMY7WpfQPVkhEuBVcCVdZ5Oqe3g/63HPwDHfPxDZu4HJh//MFf1motngMsycyIzjwALgJfaL7E1veaCiPgjqqu6B9svrXW95uJdVOtoNkXEduCnmfnz9ktsTa+5eBH4P+CU+teR1qtr31PA+4/RfjbVx+ufy8xfU62JWtnrD2o7+I/5+Icp+po86uHVbMq5yMyXM3NfRIxExK3A45n55KxU2Y4p5yIi3gKso/pvbAl6/YycBlwEfIJqYeRHI+KslutrU6+5gOoC6b+pbnnd1mZhsyEz7wVePkbXtLOz7eAf5PEPc1WvuaBeDb25PuealmtrW6+5+FPg9cDDwIeAjRFxWbvltarXXPwC2JWZezPzl8B/AG9tu8AW9ZqL1VSPK3gT8HvA2oh4Z8v1nSimnZ1tB//koxyY4vEPKyNiNCKW8MrjH+aqKeciIkaA+4DvZ+ZVmTnXn1805Vxk5rWZeX79ZtZXgM9l5rbZKLIlvX5Gvgu8JSJOq698l1Fd8c5VvebiOapngR3KzJeogu61rVd4YvghcGZEnBoRvwNcAHy714BWP9UDbAUuiYhHqR//EBEbeeXxD7dRPddnHvA39V/oXDXlXFA9fGkVsLD+FAfAdZnZ8y/zVazn98Xslta6fj8j1wEP1ed+IzPn8sVRv7m4GNgZEUeo7mv/2yzW2rqIWAe8JjPvqOflIarsvCszf9JrrI9skKTCuIBLkgrT6FbPdBYNRMTJwD3A71K9u/xnc/wjZ5L0qtL3in+ARQN/Ceyu99v9KvC3wy5akjS4Jlf8k4sGvtbV/ptFAwARMbloYAVwS33Og8Df9fsC9Qrdd+AOXJLU1G924Ko3s2qsb/DXG6u88Rhdw9pvF6rQ77lLlyTpmFYyzR0MZ/Jxzn777Xa29bMHYPPmzSxd2vMRE5IkYO/evYyNjUGdn9Mxk+D/zaIB4JdUiwZuBd5AtejiOzTYb7c2AbB06VLOOOOMGZQkScWZ9u3xaQd/v0UDEfEPwN31Pf9fUz1nRZJ0gmgU/Jn5NNXycDJzS0f7N4Fvdp17EPjA8EqUJA2TC7gkqTAGvyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8kFcbgl6TCGPySVBiDX5IKY/BLUmEMfkkqjMEvSYUx+CWpMH134IqIecDtwLnAIWBDZo7XfW8FvtBx+jJgLdV+u08CP6jbt2bmF4dYtyRpQE22XlwLjGbm8ohYBmwCrgDIzO8BFwJExAeAZzNzW0RcDHw9M//6+JQtSRpUk+BfAWwDyMydEXFe9wkRcQpwI3BB3fR24G0R8e/Az4CPZOae4ZQsSZqJJvf4FwP7O44nIqL7H4y/AP4pM/fVx08AN2TmKuCfgS/NuFJJ0lA0Cf4DwKLOMZl5uOucMeDOjuOHgW/Vr7cCvz9whZKkoWoS/DuANQD1Pf7dnZ0RsQRYmJnPdDTfCfxh/foPgO/OvFRJ0jA0uce/FbgkIh4FRoD1EbERGM/M+4GzgKe7xnwSuCsirgFeBDYMr2RJ0kz0Df7MPAJc3dX8REf/LqpP/nSO+V/gomEUKEkaLhdwSVJhDH5JKozBL0mFMfglqTAGvyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8kFcbgl6TCGPySVBiDX5IK03cjloiYB9wOnAscAjZk5nhH/23Au4EX6qYrgAXAFuBk4FlgfWYeHG7pkqRBNLniXwuMZuZyqi0VN3X1vw14T2ZeWP/aD1wPbMnMlcDjwFXDLFqSNLgmwb8C2AaQmTuB8yY76v8NnAncERE7IuLPu8cADwIXD61iSdKMNAn+xcD+juOJiJi8RXQK8CXgT4DLgGsi4pyuMS8AS4ZTriRppvre4wcOAIs6judl5uH69UHgi5P37yPiYar3AibH/Kr+/fmhVSxJmpEmV/w7gDUAEbEM2N3RdxbwSETMj4gFVLd4HuscA6wGtg+tYknSjDS54t8KXBIRjwIjwPqI2AiMZ+b9EbEZ2Am8DHw1M/8rIm4C7o6IDwP7gHXHqX5J0jT1Df7MPAJc3dX8REf/LcAtXWN+SnXPX5J0gnEBlyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8kFcbgl6TCGPySVBiDX5IKY/BLUmEMfkkqjMEvSYUx+CWpMAa/JBWm7w5cETEPuJ1qE/VDwIbMHO/o/xjwwfrwgcy8MSJGgB8D/1O3fzszrxtq5ZKkgTTZc3ctMJqZy+vN1jcBVwBExJuBMeB84CiwPSK2AgeBxzLz8uNTtiRpUE1u9awAtgFk5k7gvI6+Z4DLMnOi3pt3AfAS8Hbg9RHxrYh4ICJiyHVLkgbUJPgXA/s7jici4iSAzHw5M/dFxEhE3Ao8nplPAnuAT2fmRcDNwD3DLlySNJgmwX8AWNQ5JjMPTx5ExCiwuT7nmrr5P4H7ADLzEaqr/5GhVCxJmpEmwb8DWANQ3+PfPdlRh/l9wPcz86rMnKi7bgA+Wp9zLvCjzDw6zMIlSYNp8ubuVuCSiHgUGAHWR8RGYByYD6wCFkbE6vr864C/B+6JiPcCh4EPDbtwSdJg+gZ//abt1V3NT3S8Hp1i6HsHLUqSdPy4gEuSCmPwS1JhDH5JKozBL0mFMfglqTAGvyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8kFcbgl6TCGPySVBiDX5IK03cHroiYB9wOnAscAjZk5nhH/4eBq6i2WLwpM/8lIk4DtgAnA88C6zPz4HGoX5I0TU2u+NcCo5m5HPgksGmyIyKWAh8B3g28B/h0RCwErge2ZOZK4HGqfxgkSSeAJputrwC2AWTmzog4r6PvncCOzDwEHIqIceCceszN9TkP1q8/3+NrzAfYu3fv9KqXpEJ15OX86Y5tEvyLgf0dxxMRcVJmHj5G3wvAkq72ybZeTgcYGxtrUrMk6RWnA09NZ0CT4D8ALOo4nleH/rH6FgHPd7T/qqOtl13ASmAPMNGgJkkq3Xyq0N813YFNgn8HcDnwjYhYBuzu6PsO8KmIGAUWAmcDP6jHrAG+AqwGtvf6AvWtokemW7wkFW5aV/qTRo4ePdrzhI5P9ZwDjADrqUJ9PDPvrz/VcyXVG8U3Z+a9EfE64G6qq/19wLrMfHGQAiVJw9U3+CVJc4sLuCSpMAa/JBWmyZu7QzPIKuA262tTg7n4GPDB+vCBzLyx/Srb0W8uOs75V+C+zPzH9qtsR4Pvi9XADfXhY8BfZeacvF/bYC4+DvwxcITq/cWts1JoiyLifOAzmXlhV/vlVAtnDwN3ZeaXe/05bV/xD7IKeK7qNRdvBsaAdwHLgUsj4pxZqbIdU85Fh5uAU1utanb0+r5YBHwWeF9mLgOeBk6bjSJb0msuXkuVF8uBS4EvzEqFLYqIa4E7gdGu9gVUC2QvBVYBV9Z5OqW2g/+3VgEDx1wFnJn7gclVwHNVr7l4BrgsMycy8wiwAHip/RJb02suiIg/orqqe7D90lrXay7eRfVx6k0RsR34aWb+vP0SW9NrLl4E/g84pf51pPXq2vcU8P5jtJ9N9SnL5zLz11QfjV/Z6w9qO/iPuQp4ir4mK35fzaaci8x8OTP3RcRIRNwKPJ6ZT85Kle2Yci4i4i3AOqr/xpag18/IacBFwCeo1sd8NCLOarm+NvWaC6gukP6b6pbXbW0WNhsy817g5WN0TTs72w7+QVYBz1W95oJ6Udzm+pxrWq6tbb3m4k+B1wMPAx8CNkbEZe2W16pec/ELYFdm7s3MXwL/Aby17QJb1GsuVlOtWn0T8HvA2oh4Z8v1nSimnZ1tB//kil6mWAW8MiJGI2IJr6wCnqumnIuIGAHuA76fmVdl5lx/jMWUc5GZ12bm+fWbWV8BPpeZ22ajyJb0+hn5LvCWiDitvvJdRnXFO1f1movnqB4JcygzX6IKute2XuGJ4YfAmRFxakT8DnAB8O1eA1r9VA+wFbgkIh6lXgUcERt5ZRXwbVSPd5gH/E39FzpXTTkXVM/gWAUsrD/FAXBdZvb8y3wV6/l9Mbulta7fz8h1wEP1ud/IzLl8cdRvLi4GdkbEEar72v82i7W2LiLWAa/JzDvqeXmIKjvvysyf9Brryl1JKowLuCSpMAa/JBXG4Jekwhj8klQYg1+SCmPwS1JhDH5JKozBL0mF+X819Pl1HaMOXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20bd014f860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data preprocessing\n",
    "X_test_transf = np.zeros(test_df.shape)\n",
    "for i in range(len(test_df)):\n",
    "    X_test_transf[i,:,:,:] = Validation_data_preprocess(test_df[i])\n",
    "\n",
    "# predict results\n",
    "results_0 = model.predict(X_test_transf)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results_0,axis = 1)\n",
    "\n",
    "submission = pd.DataFrame(np.arange(1,28001),columns=[\"ImageId\"])\n",
    "submission['Label'] = results\n",
    "submission.to_csv(\"MNIST_Sirui.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
